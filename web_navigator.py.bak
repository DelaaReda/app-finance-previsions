# web_navigator.py — drop-in replacement

import os
import re
import json
import time
import random
import string
from urllib.parse import urlencode
from typing import Iterable
import logging
import argparse
import requests

# -----------------------------------------------------------------------------
# Logger (module-level). Pas de basicConfig ici; configuré dans main().
# -----------------------------------------------------------------------------
logger = logging.getLogger("web_navigator")
if not logger.handlers:
    logger.addHandler(logging.NullHandler())

# -----------------------------------------------------------------------------
# Config SearXNG
# -----------------------------------------------------------------------------
SEARXNG_FETCH_URL = "https://searx.space/data/instances.json"  # source publique
SEARXNG_TIMEOUT = (8, 20)  # (connect, read)
SEARXNG_DEFAULT_ENGINES = ["duckduckgo", "bing", "google"]  # d'abord moins bloqués
# Instances connues pour renvoyer du JSON sans CAPTCHA
SEARXNG_KNOWN_JSON_OK = [
    "https://search.buddyverse.net",
    "https://search.inetol.net",
    "https://search.bus-hit.me",
]

# Domaines à éviter (pollution type StackOverflow/GitHub quand on cherche des concurrents)
FINANCE_BLACKLIST_DOMAINS = (
    "stackoverflow.com",
    "stackexchange.com",
    "superuser.com",
    "serverfault.com",
    "github.com",
    "learn.microsoft.com",
)
SEARXNG_SAFE_PARAMS = {
    "format": "json",
    "language": "en",
    "safesearch": 0,
    "categories": "general",
}

# seed statique si searx.space est KO
SEARXNG_FALLBACK_INSTANCES = [
    "https://searx.be",
    "https://searx.tiekoetter.com",
    "https://search.inetol.net",
    "https://xo.wtf",
    "https://searx.headpat.exchange",
    "https://searxng.site",
    "https://searxng.brihx.fr",
    "https://search.bus-hit.me",
]

# Domaines favoris / à éviter pour le scoring
TRUSTED_DOMAINS = (
    "wikipedia.org", "investor.", "sec.gov", "sedar", "canada.ca", "nasdaq.com",
    "ft.com", "reuters.com", "bloomberg.com", "wsj.com", "yahoo.com", "seekingalpha.com",
    "marketbeat.com", "morningstar.", "tipranks.com", "barchart.com"
)
LOW_VALUE_DOMAINS = ("pinterest.", "reddit.com/r/", "/ads?", "utm_", "doubleclick.net")

# -----------------------------------------------------------------------------
# HTTP helpers
# -----------------------------------------------------------------------------
def _random_ua() -> str:
    v = ".".join(str(random.randint(60, 125)) for _ in range(3))
    chrome = f"Chrome/{v}"
    safari = f"Safari/{random.randint(500, 600)}.{random.randint(1, 50)}"
    return f"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) {chrome} {safari}"

def _request_json(url: str, params: dict | None = None, timeout=SEARXNG_TIMEOUT) -> dict:
    headers = {
        "User-Agent": _random_ua(),
        "Accept": "application/json,text/*;q=0.4,*/*;q=0.1",
        "Accept-Language": "en-US,en;q=0.7,fr;q=0.5",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }
    r = requests.get(url, params=params, headers=headers, timeout=timeout, allow_redirects=True)
    r.raise_for_status()
    ctype = (r.headers.get("Content-Type") or "").lower()
    if "application/json" not in ctype:
        raise ValueError(f"non-json response (Content-Type={ctype or 'unknown'})")
    return r.json()

# -----------------------------------------------------------------------------
# SearXNG instance discovery
# -----------------------------------------------------------------------------
def fetch_searxng_instances(logger_=None) -> list[str]:
    if logger_ is None:
        logger_ = logger
    try:
        data = _request_json(SEARXNG_FETCH_URL, timeout=(6, 12))
        inst = []
        if isinstance(data, dict):
            # formats possibles selon searx.space
            if "instances" in data and isinstance(data["instances"], dict):
                inst = list(data["instances"].keys())
            elif "urls" in data and isinstance(data["urls"], list):
                inst = [u for u in data["urls"] if isinstance(u, str)]
        inst = [u.rstrip("/") for u in inst if isinstance(u, str) and u.startswith("http")]
        inst = list(dict.fromkeys(inst))
        random.shuffle(inst)
        # filtrage: pas de .onion, pas de http://, pas de proxys HTML-only
        inst = [
            u for u in inst
            if ".onion" not in u
            and u.startswith("https://")
            and ".within.website" not in u
        ]
        # on pousse d'abord une shortlist confirmée JSON
        prime = [u for u in SEARXNG_KNOWN_JSON_OK if u in inst]
        rest = [u for u in inst if u not in prime]
        inst = prime + rest
        if inst:
            logger_.debug(f"SearXNG public instances fetched: {len(inst)}")
            logger_.debug(f"Sample: {inst[:5]}")
            return inst
    except Exception as e:
        logger_.debug(f"searx.space fetch error: {e}")

    fallback = [u.rstrip("/") for u in (SEARXNG_KNOWN_JSON_OK + SEARXNG_FALLBACK_INSTANCES)]
    random.shuffle(fallback)
    logger_.info(f"Using fallback SearXNG instances: {fallback[:5]}...")
    return fallback

# -----------------------------------------------------------------------------
# SearXNG search
# -----------------------------------------------------------------------------
def search_searxng(
    query: str,
    num: int = 10,
    engines: Iterable[str] = SEARXNG_DEFAULT_ENGINES,
    logger_=logger,
) -> dict:
    """
    Essaie plusieurs instances SearXNG jusqu’à obtenir des résultats.
    Retourne { 'results': [ {title,url,snippet}, ... ] } (Firecrawl-like).
    """
    instances = fetch_searxng_instances(logger_=logger_)
    base_params = {
        **SEARXNG_SAFE_PARAMS,
        "q": query,
        "count": min(max(num, 1), 50),
    }
    engines = list(engines)
    engines_param = ",".join(engines)

    for base in instances:
        url = f"{base}/search"
        try:
            # tentative #1 : avec engines
            params = dict(base_params, engines=engines_param)
            logger_.debug(f"SearXNG query on {base}: {query} | engines={engines_param}")
            data = _request_json(url, params=params)
            items = []
            for r in (data.get("results") or []):
                title = (r.get("title") or "").strip()
                url_ = (r.get("url") or "").strip()
                snippet = (r.get("content") or r.get("snippet") or "").strip()
                if not url_ or not title:
                    continue
                items.append({"title": title, "url": url_, "snippet": snippet})
            if items:
                return {"results": _rank_results(items, query)}
        except Exception as e:
            logger_.debug(f"SearXNG error on {base} (with engines): {e}")
            # tentative #2 : sans engines
            try:
                data = _request_json(url, params=base_params)
                items = []
                for r in (data.get("results") or []):
                    title = (r.get("title") or "").strip()
                    url_ = (r.get("url") or "").strip()
                    snippet = (r.get("content") or r.get("snippet") or "").strip()
                    if not url_ or not title:
                        continue
                    items.append({"title": title, "url": url_, "snippet": snippet})
                if items:
                    logger_.debug(f"Succeeded on {base} without engines param.")
                    return {"results": _rank_results(items, query)}
            except Exception as e2:
                logger_.debug(f"SearXNG error on {base} (no engines): {e2}")
                continue

    logger_.warning("All SearXNG instances failed.")
    return {"results": []}

# -----------------------------------------------------------------------------
# Quality ranking
# -----------------------------------------------------------------------------
def _domain(url: str) -> str:
    try:
        from urllib.parse import urlparse
        h = urlparse(url).netloc.lower()
        for p in ("www.", "m.", "amp."):
            if h.startswith(p):
                h = h[len(p):]
        return h
    except Exception:
        return ""

def _kw_density_score(title: str, snippet: str, q: str) -> float:
    q_tokens = [t for t in re.split(r"\W+", q.lower()) if t]
    hay = f"{title} {snippet}".lower()
    return sum(hay.count(t) for t in q_tokens) / max(len(q_tokens), 1)

def _host_score(host: str) -> float:
    s = 0.0
    if any(k in host for k in TRUSTED_DOMAINS):
        s += 0.8
    if any(k in host for k in LOW_VALUE_DOMAINS):
        s -= 0.6
    return s

def _rank_results(items: list[dict], q: str, top: int | None = None) -> list[dict]:
    """Dédup + scoring qualité + filtre bruit si finance."""
    seen = set()
    ranked = []

    finance_mode = bool(re.search(r"\b(competitors?|peers?|similar\s+stocks?)\b", q, re.I))

    for it in items:
        url = it["url"]
        host = _domain(url)
        if finance_mode and any(host.endswith(bad) or bad in host for bad in FINANCE_BLACKLIST_DOMAINS):
            continue
        key = (host, it["title"].strip().lower())
        if key in seen:
            continue
        seen.add(key)
        score = 0.0
        score += _kw_density_score(it["title"], it.get("snippet", ""), q)
        score += _host_score(host)
        ranked.append((score, it))
    ranked.sort(key=lambda x: x[0], reverse=True)
    out = [it for _, it in ranked]
    return out[:top] if top else out
# -----------------------------------------------------------------------------
# Smart search orchestrator (peers)
# -----------------------------------------------------------------------------
def smart_search(company: str, symbol: str, industry: str, num: int = 10, refresh: bool = False, logger_=logger) -> dict:
    """
    Stratégie:
      1) SearXNG (engines: google,duckduckgo,bing) sur requêtes spécialisées
      2) Si <3 résultats “bons” -> SearXNG Wikipedia-only
      3) Si toujours faible -> Serper / Tavily (si clés présentes)
    """
    queries = [
        f"{company} {symbol} competitors OR peers",
        f"{company} competitor list",
        f"{symbol} similar companies",
        f"{industry} competitors public companies US Canada",
    ]

    agg = {"results": []}
    for q in queries:
        serps = search_searxng(q, num=min(10, num), engines=SEARXNG_DEFAULT_ENGINES, logger_=logger_)
        agg["results"].extend(serps.get("results", []))

    agg["results"] = _rank_results(agg["results"], f"{company} {symbol} {industry}", top=num)

    if len(agg["results"]) < max(3, num // 3):
        logger_.debug("Few results; trying SearXNG with wikipedia engine only.")
        for q in queries:
            serps = search_searxng(q, num=min(10, num), engines=["wikipedia"], logger_=logger_)
            agg["results"].extend(serps.get("results", []))
        agg["results"] = _rank_results(agg["results"], f"{company} {symbol} {industry}", top=num)

    # fallback payants si dispo
    if len(agg["results"]) < max(3, num // 2):
        try:
            from secrets_local import SERPER_API_KEY as _SERPER  # type: ignore
        except Exception:
            _SERPER = os.getenv("SERPER_API_KEY", "")
        try:
            from secrets_local import TAVILY_API_KEY as _TAVILY  # type: ignore
        except Exception:
            _TAVILY = os.getenv("TAVILY_API_KEY", "")

        if _SERPER:
            try:
                agg["results"].extend(_search_serper(queries, num=min(10, num), api_key=_SERPER, logger_=logger_))
            except Exception as e:
                logger_.debug(f"Serper error: {e}")

        if len(agg["results"]) < max(3, num // 2) and _TAVILY:
            try:
                agg["results"].extend(_search_tavily(queries, num=min(10, num), api_key=_TAVILY, logger_=logger_))
            except Exception as e:
                logger_.debug(f"Tavily error: {e}")

        agg["results"] = _rank_results(agg["results"], f"{company} {symbol} {industry}", top=num)

    return agg

# -----------------------------------------------------------------------------
# Optional: Serper / Tavily light clients
# -----------------------------------------------------------------------------
def _search_serper(queries: list[str], num: int, api_key: str, logger_=logger) -> list[dict]:
    headers = {"X-API-KEY": api_key, "Content-Type": "application/json"}
    out = []
    for q in queries:
        try:
            r = requests.post(
                "https://google.serper.dev/search",
                headers=headers,
                json={"q": q, "num": min(10, num), "gl": "us", "hl": "en"},
                timeout=(8, 20),
            )
            r.raise_for_status()
            d = r.json()
            for it in (d.get("organic") or []):
                title = it.get("title", "")
                url = it.get("link", "")
                snippet = it.get("snippet", "")
                if title and url:
                    out.append({"title": title, "url": url, "snippet": snippet})
        except Exception as e:
            logger_.debug(f"Serper request failed: {e}")
    return out

def _search_tavily(queries: list[str], num: int, api_key: str, logger_=logger) -> list[dict]:
    out = []
    for q in queries:
        try:
            r = requests.post(
                "https://api.tavily.com/search",
                headers={"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"},
                json={"api_key": api_key, "query": q, "max_results": min(10, num), "search_depth": "basic"},
                timeout=(8, 20),
            )
            r.raise_for_status()
            d = r.json()
            for it in (d.get("results") or []):
                title = it.get("title", "")
                url = it.get("url", "")
                snippet = it.get("content", "")
                if title and url:
                    out.append({"title": title, "url": url, "snippet": snippet})
        except Exception as e:
            logger_.debug(f"Tavily request failed: {e}")
    return out

# -----------------------------------------------------------------------------
# CLI
# -----------------------------------------------------------------------------
def _set_log_level(level_str: str):
    level = getattr(logging, level_str.upper(), logging.INFO)
    root = logging.getLogger()
    if not root.handlers:
        logging.basicConfig(
            level=level,
            format="%(asctime)s | %(levelname)-7s | %(name)s | %(message)s",
            datefmt="%H:%M:%S",
        )
    else:
        root.setLevel(level)
    logger.setLevel(level)

def main():
    parser = argparse.ArgumentParser(description="Lightweight web navigator with SearXNG + optional Serper/Tavily.")
    g = parser.add_mutually_exclusive_group(required=True)
    g.add_argument("--engine", choices=["searxng"], help="Moteur brut (actuellement: searxng)")
    g.add_argument("--smart", action="store_true", help="Recherche intelligente (peers)")

    parser.add_argument("--q", help="Query texte (avec --engine)")
    parser.add_argument("--n", type=int, default=10, help="Nombre de résultats")
    parser.add_argument("--log", default="INFO", choices=["DEBUG","INFO","WARNING","ERROR","CRITICAL"], help="Niveau de log")
    parser.add_argument("--refresh", action="store_true", help="Ignorer le cache (placeholder)")

    # smart params
    parser.add_argument("--symbol", help="Ticker (smart mode)")
    parser.add_argument("--company", help="Nom société (smart mode)")
    parser.add_argument("--industry", help="Industrie (smart mode)")

    args = parser.parse_args()
    _set_log_level(args.log)

    if args.engine:
        if args.engine == "searxng":
            if not args.q:
                parser.error("--q est requis avec --engine searxng")
            out = search_searxng(query=args.q, num=args.n, engines=SEARXNG_DEFAULT_ENGINES, logger_=logger)
            print(json.dumps(out, indent=2, ensure_ascii=False))
            return

    if args.smart:
        if not (args.symbol and args.company and args.industry):
            parser.error("--smart requiert --symbol, --company et --industry")
        out = smart_search(company=args.company, symbol=args.symbol, industry=args.industry, num=args.n, refresh=args.refresh, logger_=logger)
        print(json.dumps(out, indent=2, ensure_ascii=False))
        return

if __name__ == "__main__":
    main()